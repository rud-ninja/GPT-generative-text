{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BByeVo127LzH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHAaqEHbKtVo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "0eb3a2f9-e5e3-410b-ee59-f99f662939af"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "import sys\n",
        "import time\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tokenizers import ByteLevelBPETokenizer, Tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "k9GRFsT_Kw6y",
        "outputId": "14266540-a088-497a-b836-f19caaa2722c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_file(\"/content/drive/MyDrive/bpe.tokenizer.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mnNHERCsK8fK",
        "outputId": "29a6c5b3-905d-4d0b-a61b-ebdf9f0d27ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "block_size = 320\n",
        "batch_size = 64\n",
        "n_embd = 512\n",
        "ma_head = 16\n",
        "n_blocks = 3\n",
        "learning_rate = 1e-3\n",
        "max_iters = 5001\n",
        "eval_interval = 500\n",
        "device = \"cuda\" if  torch.cuda.is_available() else \"cpu\"\n",
        "eval_iters = 100\n",
        "dropout = 0.25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "I9j_0zpoK8bi",
        "outputId": "a0118572-79e8-4562-a90a-aaf9e01bc310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(9)\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4*n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*n_embd, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=ma_head) for _ in range(n_blocks)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_embd = self.token_embedding_table(idx)\n",
        "        pos_embd = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_embd + pos_embd\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets==None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        def print_progressively_dynamic(text, delay=0.05):\n",
        "            for char in text:\n",
        "                sys.stdout.write(char)\n",
        "                sys.stdout.flush()\n",
        "                time.sleep(delay)\n",
        "\n",
        "        def update_text_and_print(new_text):\n",
        "            print_progressively_dynamic(new_text)\n",
        "            # print()\n",
        "\n",
        "        update_text_and_print(tokenizer.decode(idx[0].tolist()))\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "            update_text_and_print(tokenizer.decode(idx_next[0].tolist()))\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/llmodel2.pt\", map_location=torch.device(device)))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fBv4784JK8Yk",
        "outputId": "89f80be7-3624-43d8-a060-c778893a1aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BigramLanguageModel(\n",
              "  (token_embedding_table): Embedding(1000, 512)\n",
              "  (position_embedding_table): Embedding(320, 512)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=512, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=512, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=512, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.25, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.25, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (3): Dropout(p=0.25, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=512, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=512, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=512, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.25, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.25, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (3): Dropout(p=0.25, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=512, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=512, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=512, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.25, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.25, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (3): Dropout(p=0.25, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_input = \"Things get better when\"\n",
        "context = torch.tensor(tokenizer.encode(my_input).ids, dtype=torch.long).view(1, -1)\n",
        "model.generate(context, max_new_tokens=200)\n",
        "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "# print(tokenizer.decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "xQXOBLDGLSlx",
        "outputId": "addc832b-c951-44ce-a320-81fa5344304e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Things get better when a general population of 40 per cent of the majority . The majority of the Family  was 15 per cent in the Great Buddhist connected MDOT horsepower ( including outside Wiki ) , and in DDOT modified by New South Wales . \n",
            " The buddhist contribution lightly  to the  and western cost of the U.S. authorities 4  ,  8005 defining at least turnpoints about 1  ,  500 by 1  ,  500  ,  900 pm in flight , and as of April 1997 authorities are located in North Wales . From through renovations of April 2010 ,"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "\n",
        "def print_progressively_dynamic(text, delay=0.05):\n",
        "    for char in text:\n",
        "        sys.stdout.write(char)\n",
        "        sys.stdout.flush()\n",
        "        time.sleep(delay)\n",
        "\n",
        "def update_text_and_print(new_text):\n",
        "    print_progressively_dynamic(new_text)\n",
        "    print()\n",
        "\n",
        "update_text_and_print(tokenizer.decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "f7NlHfXDPVmf",
        "outputId": "77264259-0859-4b3f-b66f-17967882515e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "why are we here? . Among these investments must import themselves in the heart of the show , we have many was lives from mostly rowing community . These anthems are only weight , that studied rowing , and the attacks are outtrowing . The couple at\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bY4NayyNSIBl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}